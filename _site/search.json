[
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "LiDAR Toolset for Forest Monitoring & Stand Mensuration",
    "section": "",
    "text": "VM0048: Jurisdictional REDD+ Methodology\n\n\nGuide to Verra’s VM0048 consolidated methodology and VMD0055-compliant emissions reporting, including baseline calculation, jurisdictional deforestation risk mapping, and leakage analysis.\n\n\n\n\n\nJan 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nVM0047: ARR Project Feasibility\n\n\nFeasibility assessment framework for Afforestation, Reforestation, and Revegetation (ARR) projects under VM0047 methodology. This resource provides standardized protocol for evaluating land degradation, forest change detection algorithms, and baseline establishment procedures, including data processing workflows for satellite imagery analysis and soil carbon stock potential to support project developers in initial feasibility screening.\n\n\n\n\n\nOct 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nVM0010: IFM Reporting Template\n\n\nReporting template for Improved Forest Management (IFM) projects under VM0010 methodology. Provides standardized procedures for forest inventory data collection, carbon stock calculations, emission reduction quantification, and MRV reporting requirements. Includes field measurement protocols, allometric equation selection guidance, statistical sampling design, and automated calculation templates ensuring VCS compliance for commercial forest carbon projects.\n\n\n\n\n\nNov 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nVT0007: JREDD+ Risk Mapping Scripts\n\n\nRisk assessment toolkit for Jurisdictional REDD+ programs implementing VT0007 methodology. Features geospatial analysis scripts for identifying and mapping deforestation risk factors, including proximity to infrastructure, agricultural pressure zones, and historical forest loss patterns. Integrates multiple satellite data sources (Landsat, Sentinel-2, MODIS) with socioeconomic indicators to generate probabilistic risk surfaces supporting strategic intervention planning and monitoring system design for jurisdictional-scale forest conservation programs.\n\n\n\n\n\nNov 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nART-TREES: Repository Demo\n\n\nDemonstration repository showcasing version control best practices for greenhouse gas calculation workflows under ART-TREES methodology. Features standardized Git workflows, automated testing frameworks, and documentation templates ensuring reproducible carbon accounting processes. Includes branching strategies for collaborative development, continuous integration pipelines for calculation validation, and release management procedures supporting transparent and auditable REDD+ project development.\n\n\n\n\n\nAug 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nART-TREES: Allometry Validation\n\n\nValidation and calibration procedures for allometric biomass models compliant with ART-TREES requirements. Technical guidance in model selection methodology for avoiding Types 1-4 statistical errors, bias corrections and hyperparameter tuning.\n\n\n\n\n\nFeb 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nART-TREES: Monte Carlo Tools\n\n\nMonte Carlo simulation methods for estimating uncertainty of REDD+ emission factors and activity data in compliance with ART-TREES Standard V2.0. Features workflows for biomass estimation uncertainty analysis, allometric model error propagation, and activity data variance assessment using 10,000-iteration simulations with 90% confidence intervals. Includes specialized modules for cross-validation bias detection, spatial uncertainty modeling, and automated uncertainty deduction calculations that also incorporate best practices from CEOS LPV Biomass Protocol.\n\n\n\n\n\nDec 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMapping Templates: Site Visualization\n\n\nCollection of interactive cartographic templates and visualization frameworks for forest carbon project mapping. Provides symbology, layout templates, and map generation for site locator, project boundary and watershed delineation.\n\n\n\n\n\nDec 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLiDAR: Forest Height Analysis\n\n\nAdvanced forest structure analysis toolkit combining LiDAR and photogrammetric data for operational forest inventory planning. Features automated tree top detection algorithms, canopy height model generation, and stand structure characterization supporting efficient field plot allocation and sampling design. Includes crown delineation protocols, diameter-height relationship modeling, and accessibility analysis for optimizing field crew deployment in complex terrain environments.\n\n\n\n\n\nOct 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWetland Mapping: SAR & Landsat Workflows\n\n\nFramework for mapping endorheic wetland dynamics, combining Sentinel-1 InSAR processing with Landsat time series analysis (1994-2015), using water extraction indices (NDWI, MNDWI, AWEIsh) and spectral mixture analysis for sub-pixel water detection. Community mapping components include key informant interviews, focus group discussions, and rapid participatory appraisals capturing local ecological knowledge of seasonal fishing migration patterns and traditional resource management practices.\n\n\n\n\n\nJun 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGeoODK Survey: Open Source Build\n\n\nOpen-source mobile data collection framework built on Open Data Kit (GeoODK) architecture for socioeconomic field surveys. Provides customizable survey templates, GPS-enabled data collection forms, with automated quality control protocols, multilingual support, and integrated statistical sampling tools using android mobile devices\n\n\n\n\n\nFeb 5, 2018\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "About",
      "Teaching"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Home",
      "About",
      "Bio"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LiDAR Forest Tools & Stand Mensuration",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nReferences\n\nRoussel, Jean-Romain, David Auty, Nicholas C. Coops, Piotr Tompalski, Tristan R. H. Goodbody, Andrew Sánchez Meador, Jean-François Bourdon, Florian de Boissieu, and Alexis Achim. 2020. “lidR: An r Package for Analysis of Airborne Laser Scanning (ALS) Data” 251: 112061. https://doi.org/10.1016/j.rse.2020.112061.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "01-point-cloud.html",
    "href": "01-point-cloud.html",
    "title": "LiDAR Point Cloud Processing",
    "section": "",
    "text": "This document outlines the initial, computationally-intensive steps of processing raw LiDAR point cloud data into usable rasters for forest mensuration. The workflow focuses on ground segmentation and point classification. Commissioned by BC Timber Sales as part of the TCC Enhanced Forest Inventory work program, this pipeline focuses on data processing tasks essential to downstream forest mensuration and ecological analyses.\n\n\n\nThe initial challenge in large-scale LiDAR processing is managing extensive datasets that often exceed available system memory. This workflow leverages the lidR package and its core innovation (Roussel et al. 2020a), the LAScatalog object, to address this challenge.\nA LAScatalog serves as a high-level representation of a collection of .las or .laz files, enabling the batch processing of a complete LiDAR coverage without loading all points into memory. The LAScatalog operates by dividing the dataset into arbitrarily defined regions of interest (ROIs) called “chunks”. It then seamlessly loops through each chunk, either sequentially or in parallel, to perform the necessary processing. This approach allows for the efficient processing of even tiny ROIs, regardless of whether they align with original file boundaries or if the files themselves are too large to fit in memory. After each chunk is processed, the individual outputs are automatically merged to create a final, continuous, wall-to-wall result.\nIt is important to note that LAScatalog does not natively support datasets with overlapping files. When working with such data, it is a recommended practice to filter any overlaps, for example, by utilizing the withheld attribute to ensure accurate and reliable processing. Essentially, LAScatalog handles large data processing by:\n\nDefining chunks: It partitions the entire dataset into arbitrarily defined regions of interest (ROIs).\nSeamless Looping: It processes chunks sequentially or in parallel, regardless of individual file boundaries. This allows the lidR package to process tiny ROIs even if the original files are too large to fit into memory.\nMerging Outputs: Once all chunks are processed, the outputs are merged to create a continuous, wall-to-wall result.\n\n\n\n\nWe tested lidR operations using a single chunk of 311 LiDAR tiles of the Ahbau Lake district in Northern Cariboo. Initial attempts at catalog indexing and chunk processing were made using the catalog_laxindex and catalog_retile operations, but these were found to be a little more sluggish and buggy. The more efficient workflow was achieved by deploying the internal lidR tools of readALSLAScatalog, which was faster and offered simultaneous validation tools for rebuffering, spatial indexing, and filtering directly during the read-in process.\nInitial validation of the catalog tiling reported overlapping areas, a known issue as LAScatalog does not natively support datasets with overlapping files. To resolve this and avoid edge artifacts, recommended buffering was set at 10m, and the -drop_class 19 filter was applied to remove points flagged as overlaps. A custom R function was also used to handle .7z archive files, showcasing the ability to manage diverse data formats.\n\n# Read and validate LAS files, then assemble them into a LAScatalog object.\nlas_ctg_ahbau = readALSLAScatalog(\"./Data/Ahbau/Las_v12_ASPRS/\", select = \"xyzcr\")\n# A custom R function was used to handle .7z archives.\nun7zip = function(archive, where) {\n  archive &lt;- normalizePath(archive)\n  current_path &lt;- setwd(where)\n  system(paste(\"7zr x\", archive, sep = \" \"))\n  setwd(current_path)\n}\nun7zip(zip_file_ahbau_sub, zip_dir_ahbau_top)\n# Read and validate LAS files, then assemble them into a LAScatalog object.\nlas_ctg_ahbau_indexed = readALSLAScatalog(\"./Data/Ahbau/Las_v12_ASPRS/\")\nopt_select(las_ctg_ahbau_indexed) = \"xyzcr\"\n# Apply the '-drop_class 19' filter to remove overlapping points.\nopt_filter(las_ctg_ahbau_indexed) = '-drop_class 19'\n# Set chunking options for memory efficiency.\nopt_chunk_size(las_ctg_ahbau_indexed) = 1000\nopt_chunk_buffer(las_ctg_ahbau_indexed) = 10\nfilter_duplicates(las_ctg_ahbau_indexed)\nis.indexed(las_ctg_ahbau_indexed)\nlas_check(las_ctg_ahbau_indexed)\nplot(las_ctg_ahbau_indexed, chunk = TRUE)\n\n\n \n\n\n\n\n\n\nFor visualization purposes, a single tile (093g030122ne) was plotted and flagged for all subsequent illustrations.\nlas_tile_ahbau = readLAS(\"./Data/Ahbau/Las_v12_ASPRS/093g030122ne.las\", select = 'xyzcr', filter = '-drop_class 19')\nplot(las_tile_ahbau, bg = \"white\")",
    "crumbs": [
      "Home",
      "LiDAR Point Clouds"
    ]
  },
  {
    "objectID": "01-point-cloud.html#introduction",
    "href": "01-point-cloud.html#introduction",
    "title": "LiDAR Point Cloud Processing",
    "section": "",
    "text": "This document outlines the initial, computationally-intensive steps of processing raw LiDAR point cloud data into usable rasters for forest mensuration. The workflow focuses on ground segmentation and point classification. Commissioned by BC Timber Sales as part of the TCC Enhanced Forest Inventory work program, this pipeline focuses on data processing tasks essential to downstream forest mensuration and ecological analyses.\n\n\n\nThe initial challenge in large-scale LiDAR processing is managing extensive datasets that often exceed available system memory. This workflow leverages the lidR package and its core innovation (Roussel et al. 2020a), the LAScatalog object, to address this challenge.\nA LAScatalog serves as a high-level representation of a collection of .las or .laz files, enabling the batch processing of a complete LiDAR coverage without loading all points into memory. The LAScatalog operates by dividing the dataset into arbitrarily defined regions of interest (ROIs) called “chunks”. It then seamlessly loops through each chunk, either sequentially or in parallel, to perform the necessary processing. This approach allows for the efficient processing of even tiny ROIs, regardless of whether they align with original file boundaries or if the files themselves are too large to fit in memory. After each chunk is processed, the individual outputs are automatically merged to create a final, continuous, wall-to-wall result.\nIt is important to note that LAScatalog does not natively support datasets with overlapping files. When working with such data, it is a recommended practice to filter any overlaps, for example, by utilizing the withheld attribute to ensure accurate and reliable processing. Essentially, LAScatalog handles large data processing by:\n\nDefining chunks: It partitions the entire dataset into arbitrarily defined regions of interest (ROIs).\nSeamless Looping: It processes chunks sequentially or in parallel, regardless of individual file boundaries. This allows the lidR package to process tiny ROIs even if the original files are too large to fit into memory.\nMerging Outputs: Once all chunks are processed, the outputs are merged to create a continuous, wall-to-wall result.\n\n\n\n\nWe tested lidR operations using a single chunk of 311 LiDAR tiles of the Ahbau Lake district in Northern Cariboo. Initial attempts at catalog indexing and chunk processing were made using the catalog_laxindex and catalog_retile operations, but these were found to be a little more sluggish and buggy. The more efficient workflow was achieved by deploying the internal lidR tools of readALSLAScatalog, which was faster and offered simultaneous validation tools for rebuffering, spatial indexing, and filtering directly during the read-in process.\nInitial validation of the catalog tiling reported overlapping areas, a known issue as LAScatalog does not natively support datasets with overlapping files. To resolve this and avoid edge artifacts, recommended buffering was set at 10m, and the -drop_class 19 filter was applied to remove points flagged as overlaps. A custom R function was also used to handle .7z archive files, showcasing the ability to manage diverse data formats.\n\n# Read and validate LAS files, then assemble them into a LAScatalog object.\nlas_ctg_ahbau = readALSLAScatalog(\"./Data/Ahbau/Las_v12_ASPRS/\", select = \"xyzcr\")\n# A custom R function was used to handle .7z archives.\nun7zip = function(archive, where) {\n  archive &lt;- normalizePath(archive)\n  current_path &lt;- setwd(where)\n  system(paste(\"7zr x\", archive, sep = \" \"))\n  setwd(current_path)\n}\nun7zip(zip_file_ahbau_sub, zip_dir_ahbau_top)\n# Read and validate LAS files, then assemble them into a LAScatalog object.\nlas_ctg_ahbau_indexed = readALSLAScatalog(\"./Data/Ahbau/Las_v12_ASPRS/\")\nopt_select(las_ctg_ahbau_indexed) = \"xyzcr\"\n# Apply the '-drop_class 19' filter to remove overlapping points.\nopt_filter(las_ctg_ahbau_indexed) = '-drop_class 19'\n# Set chunking options for memory efficiency.\nopt_chunk_size(las_ctg_ahbau_indexed) = 1000\nopt_chunk_buffer(las_ctg_ahbau_indexed) = 10\nfilter_duplicates(las_ctg_ahbau_indexed)\nis.indexed(las_ctg_ahbau_indexed)\nlas_check(las_ctg_ahbau_indexed)\nplot(las_ctg_ahbau_indexed, chunk = TRUE)\n\n\n \n\n\n\n\n\n\nFor visualization purposes, a single tile (093g030122ne) was plotted and flagged for all subsequent illustrations.\nlas_tile_ahbau = readLAS(\"./Data/Ahbau/Las_v12_ASPRS/093g030122ne.las\", select = 'xyzcr', filter = '-drop_class 19')\nplot(las_tile_ahbau, bg = \"white\")",
    "crumbs": [
      "Home",
      "LiDAR Point Clouds"
    ]
  },
  {
    "objectID": "01-point-cloud.html#method",
    "href": "01-point-cloud.html#method",
    "title": "LiDAR Point Cloud Processing",
    "section": "2. Method",
    "text": "2. Method\n\nGround Point Classification\nGround point classification is a critical step that isolates the points representing the terrain from those representing vegetation and other objects. This process is computationally demanding, especially with larger tile collections. This workflow compares two distinct algorithms to assess their performance and resulting data quality. The RCSF package was installed from CRAN and loaded into the repository, as it was found to facilitate the process with fewer bugs.\n\nCloth Simulation Filter (CSF): The Cloth Simulation Filter algorithm (csf()) was fitted with specific parameters to account for variable topography across the study site and reduce post-processing errors (W. Zhang et al. 2016). This included the sloop_smoothparameter applied over a cloth resolution of 10cm with a rigidness factor of 1. This method produced a visually superior result for the final DTM.\nProgressive Morphological Filter (PMF): The pmf() algorithm was also tested. As the following operations demonstrate, the pmf() tool produced notably grainier elevation model. This speaks to similar accounts documented in recent literature, which suggests that this algorithm, which was originally designed for raster-based operations, under-performs when tasked with full stream point-cloud workflows [K. Zhang et al. (2003)](Roussel et al. 2020b) .\n\n\n# Apply the Cloth Simulation Filter (CSF) algorithm\nlibrary(RCSF)\nopt_output_files(las_ctg_ahbau_indexed) =  paste0(tempdir(), \"./Data/las_ctg_ahbau_csf\")\nlas_ctg_ahbau_csf = classify_ground(las_ctg_ahbau_indexed, csf(sloop_smooth=TRUE, 0.5, 1))\n\n# Apply the Progressive Morphological Filter (PMF) algorithm\nutil_makeZhangParam()\nlas_ctg_ahbau_pmf = classify_ground(las_ctg_ahbau_indexed, pmf(seq(5, 9, 13), seq(3, 3, 3)))\n\n## Metrics computation: [=-------------------------------------------------] 3% (1 threads)Metrics computation: [==------------------------------------------------] 4% (1 threads)Metrics computation: [==------------------------------------------------] 5% (1 threads)Metrics computation: [===-----------------------------------------------] 6% (1 threads)Metrics computation: [===-----------------------------------------------] 7% (1 threads)Metrics computation: [====----------------------------------------------] 8% (1 threads)Metrics computation: [====----------------------------------------------] 9% (1 threads)Metrics computation: [=====---------------------------------------------] 10% (1 threads)Metrics computation: [=====---------------------------------------------] 11% (1 threads)Metrics computation: [======--------------------------------------------] 12% (1 threads)Metrics computation: [======--------------------------------------------] 13% (1 threads)Metrics computation: [=======-------------------------------------------] 14% (1 threads)Metrics computation: [=======-------------------------------------------] 15% (1 threads)Metrics computation: [========------------------------------------------] 16% (1 threads)Metrics computation: [========------------------------------------------] 17% (1 threads)Metrics computation: [=========-----------------------------------------] 18% (1 threads)Metrics computation: [=========-----------------------------------------] 19% (1 threads)Metrics computation: [==========----------------------------------------] 20% (1 threads)Metrics computation: [==========----------------------------------------] 21% (1 threads)Metrics computation: [===========---------------------------------------] 22% (1 threads)Metrics computation: [===========---------------------------------------] 23% (1 threads)Metrics computation: [============--------------------------------------] 24% (1 threads)Metrics computation: [============--------------------------------------] 25% (1 threads)Metrics computation: [=============-------------------------------------] 26% (1 threads)Metrics computation: [=============-------------------------------------] 27% (1 threads)Metrics computation: [==============------------------------------------] 28% (1 threads)Metrics computation: [==============------------------------------------] 29% (1 threads)Metrics computation: [===============-----------------------------------] 30% (1 threads)Metrics computation: [===============-----------------------------------] 31% (1 threads)Metrics computation: [================----------------------------------] 32% (1 threads)Metrics computation: [================----------------------------------] 33% (1 threads)Metrics computation: [=================---------------------------------] 34% (1 threads)Metrics computation: [=================---------------------------------] 35% (1 threads)Metrics computation: [==================--------------------------------] 36% (1 threads)Metrics computation: [==================--------------------------------] 37% (1 threads)Metrics computation: [===================-------------------------------] 38% (1 threads)Metrics computation: [===================-------------------------------] 39% (1 threads)Metrics computation: [====================------------------------------] 40% (1 threads)Metrics computation: [====================------------------------------] 41% (1 threads)Metrics computation: [=====================-----------------------------] 42% (1 threads)Metrics computation: [=====================-----------------------------] 43% (1 threads)Metrics computation: [======================----------------------------] 44% (1 threads)Metrics computation: [======================----------------------------] 45% (1 threads)Metrics computation: [=======================---------------------------] 46% (1 threads)Metrics computation: [=======================---------------------------] 47% (1 threads)Metrics computation: [========================--------------------------] 48% (1 threads)Metrics computation: [========================--------------------------] 49% (1 threads)Metrics computation: [=========================-------------------------] 50% (1 threads)Metrics computation: [=========================-------------------------] 51% (1 threads)Metrics computation: [==========================------------------------] 52% (1 threads)Metrics computation: [==========================------------------------] 53% (1 threads)Metrics computation: [===========================-----------------------] 54% (1 threads)Metrics computation: [===========================-----------------------] 55% (1 threads)Metrics computation: [============================----------------------] 56% (1 threads)Metrics computation: [============================----------------------] 57% (1 threads)Metrics computation: [=============================---------------------] 58% (1 threads)Metrics computation: [=============================---------------------] 59% (1 threads)Metrics computation: [==============================--------------------] 60% (1 threads)Metrics computation: [==============================--------------------] 61% (1 threads)Metrics computation: [===============================-------------------] 62% (1 threads)Metrics computation: [===============================-------------------] 63% (1 threads)Metrics computation: [================================------------------] 64% (1 threads)Metrics computation: [================================------------------] 65% (1 threads)Metrics computation: [=================================-----------------] 66% (1 threads)Metrics computation: [=================================-----------------] 67% (1 threads)Metrics computation: [==================================----------------] 68% (1 threads)Metrics computation: [==================================----------------] 69% (1 threads)Metrics computation: [===================================---------------] 70% (1 threads)Metrics computation: [===================================---------------] 71% (1 threads)Metrics computation: [====================================--------------] 72% (1 threads)Metrics computation: [====================================--------------] 73% (1 threads)Metrics computation: [=====================================-------------] 74% (1 threads)Metrics computation: [=====================================-------------] 75% (1 threads)Metrics computation: [======================================------------] 76% (1 threads)Metrics computation: [======================================------------] 77% (1 threads)Metrics computation: [=======================================-----------] 78% (1 threads)Metrics computation: [=======================================-----------] 79% (1 threads)Metrics computation: [========================================----------] 80% (1 threads)Metrics computation: [========================================----------] 81% (1 threads)Metrics computation: [=========================================---------] 82% (1 threads)Metrics computation: [=========================================---------] 83% (1 threads)Metrics computation: [==========================================--------] 84% (1 threads)Metrics computation: [==========================================--------] 85% (1 threads)Metrics computation: [===========================================-------] 86% (1 threads)Metrics computation: [===========================================-------] 87% (1 threads)Metrics computation: [============================================------] 88% (1 threads)Metrics computation: [============================================------] 89% (1 threads)Metrics computation: [=============================================-----] 90% (1 threads)Metrics computation: [=============================================-----] 91% (1 threads)Metrics computation: [==============================================----] 92% (1 threads)Metrics computation: [==============================================----] 93% (1 threads)Metrics computation: [===============================================---] 94% (1 threads)Metrics computation: [===============================================---] 95% (1 threads)Metrics computation: [================================================--] 96% (1 threads)Metrics computation: [================================================--] 97% (1 threads)Metrics computation: [=================================================-] 98% (1 threads)Metrics computation: [=================================================-] 99% (1 threads)Metrics computation: [==================================================] 100% (1 threads)\n\n\n\nNoise Removal\nOnce ground points are classified, any remaining noise, such as atmospheric interference or photon noise, is identified and removed to ensure data integrity. This process was carried out using the Statistical Outlier Removal (sor) algorithm, which was fitted with a default neighborhood sample of k=10 and a multiplier of m=3.\nThe workflow considered four potential methods for noise screening, including:\n\nUsing internal algorithms with the filter_poi function and a LASNOISE string.\nFiltering with opt_filter and the -drop class 19 string.\nScreening by a Z-axis threshold (e.g., Z &gt; 40 & Z &lt; 0).\nScreening by the 95th percentile.\n\nThe approach of using filter_poi with an internal algorithm was found to be slower in the long run due to a loss of spatial indexing after a new catalog object was assigned.\n\nopt_output_files(las_ctg_ahbau_csf) = paste0(tempdir(), \"./Data/las_ctg_ahbau_csf_so\")\nopt_select(las_ctg_ahbau_csf) = \"xyzcr\"\nopt_filter(las_ctg_ahbau_csf) = \"-drop_class 19\"\nlas_ctg_ahbau_csf_so = classify_noise(las_ctg_ahbau_csf, sor(k=10, m=3))\nlas_ctg_ahbau_csf_sor = filter_poi(las_ctg_ahbau_csf_so, Classification != LASNOISE)\n\n\n \n\n\n\n\nDigital Terrain Model (DTM)\nThis section outlines a crucial rasterization step in terrain analysis: deriving a DTM from the classified ground points. This model is a fundamental input for all subsequent height-based analyses. The Inverse Distance Weighting (knnidw) algorithm was chosen for this task due to its efficiency and robustness, as well as its sensitivity to lake anomalies (Tu et al. 2020). The knnidw parameters were set to a default maximum radius of 50m, a neighborhood of 10, an inverse distance weighting power of 2, and a resolution of 1m.\nA DTM was derived from the continuous, cleaned point cloud by visually comparing the output of the CSF and PMF processing. This visual assessment confirmed that the Cloth Simulation Filter (CSF) produced a better DEM result than the PMF, which appeared grainy. This is a key finding, as lidR operations are point-oriented, making csf a more suitable choice than the raster-based PMF algorithm.\n\nlas_tile_ahbau_csf_sor_dtm = grid_terrain(las_tile_ahbau_csf_sor, 1, knnidw(10, 2, 50))\nlas_tile_ahbau_pmf_sor_dtm = grid_terrain(las_tile_ahbau_pmf_sor, 1, knnidw(10, 2, 50))\nlas_tile_ahbau_csf_sor_dtm_plot = plot_dtm3d(las_tile_ahbau_csf_sor_dtm, bg = \"white\") \nlas_tile_ahbau_pmf_sor_dtm_plot = plot_dtm3d(las_tile_ahbau_pmf_sor_dtm, bg = \"white\") \n\n\n \n\n\nVisually, the cloth simulation filter produced a sharper looking elevation model, which appears noticeably grainy. The former output was nomianted as the candidate model to proceed in DEM preparations across the rest of the Ahbau catalog.1 In the following, the csf raster rendered in a hillshade visualization, highlighting the terrain feature beneath.\n\n# Define LAScatalog options for DTM generation\nopt_output_files(las_ctg_ahbau_csf_sor) = paste0(tempdir(), \"./Data/las_ctg_ahbau_dtm\")\nopt_select(las_ctg_ahbau_csf_sor) = \"xyzcr\"\nopt_filter(las_ctg_ahbau_csf_sor) = '-drop_class 19'\nopt_chunk_size(las_ctg_ahbau_csf_sor) = 1000\nopt_chunk_buffer(las_ctg_ahbau_csf_sor) = 10\nsensor(las_ctg_ahbau_csf_sor) = 'als'\nindex(las_ctg_ahbau_csf_sor) = \"quadtree\"\n\n# Generate the DTM from the classified point cloud\nlas_ctg_ahbau_csf_dtm = grid_terrain(las_ctg_ahbau, 1, knnidw())\n\n# Create a hillshade visualization from the DTM\nlas_ctg_ahbau_csf_sor_dtm_crop = crop(las_ctg_ahbau_csf_dtm, extent(las_ctg_ahbau_csf_dtm) - 10)\ncrs(las_ctg_ahbau_csf_sor_dtm_crop) = 3005\nlas_ctg_ahbau_csf_sor_dtm_slope = terra::terrain(las_ctg_ahbau_csf_sor_dtm_crop, \"slope\", unit = \"radians\")\nlas_ctg_ahbau_csf_sor_dtm_aspect = terra::terrain(las_ctg_ahbau_csf_sor_dtm_crop, \"aspect\", unit = \"radians\")\nlas_ctg_ahbau_csf_sor_dtm_shade = hillShade(las_ctg_ahbau_csf_sor_dtm_slope, las_ctg_ahbau_csf_sor_dtm_aspect, 40, 270)\nplot(las_ctg_ahbau_csf_sor_dtm_shade, col=grey(0:100/100), legend=FALSE)\n\n\n\n\n\n\n\nHeight Normalization\nThe final step in preparing the point cloud for canopy analysis is height normalization. This process uses the newly created DTM to transform the point cloud’s Z values from absolute elevation to height above ground level. A height-normalized point cloud is the essential input for all subsequent canopy-related analyses, such as individual tree detection and canopy height modeling.\n#| warning: false\n#| message: false\n#| error: false\n#| echo: true\n#| eval: false\nopt_output_files(las_ctg_ahbau_csf_sor) =  paste0(tempdir(), \"./Data/las_ctg_ahbau_norm\")\nopt_select(las_ctg_ahbau_csf_sor) = \"xyzr\"\nopt_filter(las_ctg_ahbau_csf_sor) = '-keep_first' \nopt_chunk_size(las_ctg_ahbau_csf_sor) = 1000 \nopt_chunk_buffer(las_ctg_ahbau_csf_sor) = 10\nsensor(las_ctg_ahbau_csf_sor) = 'als'\nindex(las_ctg_ahbau_csf_sor) = \"quadtree\"\n\nlas_ctg_ahbau_csf_sor_norm = normalize_height(las_ctg_ahbau_csf_sor, knnidw())\nlas_tile_ahbau_csf_sor_norm = normalize_height(las_tile_ahbau_csf_sor, knnidw())\nhist(filter_ground(las_tile_ahbau_csf_sor_norm)$Z, \n     breaks = seq(-0.6, 0.6, 0.01), main = \"\", xlab = \"Elevation\")\nplot(las_tile_ahbau_csf_sor_norm, bg = \"white\")\n\n\n\n## Inverse distance weighting: [=====---------------------------------------------] 10% (1 threads)Inverse distance weighting: [=====---------------------------------------------] 11% (1 threads)Inverse distance weighting: [======--------------------------------------------] 12% (1 threads)Inverse distance weighting: [======--------------------------------------------] 13% (1 threads)Inverse distance weighting: [=======-------------------------------------------] 14% (1 threads)Inverse distance weighting: [=======-------------------------------------------] 15% (1 threads)Inverse distance weighting: [========------------------------------------------] 16% (1 threads)Inverse distance weighting: [========------------------------------------------] 17% (1 threads)Inverse distance weighting: [=========-----------------------------------------] 18% (1 threads)Inverse distance weighting: [=========-----------------------------------------] 19% (1 threads)Inverse distance weighting: [==========----------------------------------------] 20% (1 threads)Inverse distance weighting: [==========----------------------------------------] 21% (1 threads)Inverse distance weighting: [===========---------------------------------------] 22% (1 threads)Inverse distance weighting: [===========---------------------------------------] 23% (1 threads)Inverse distance weighting: [============--------------------------------------] 24% (1 threads)Inverse distance weighting: [============--------------------------------------] 25% (1 threads)Inverse distance weighting: [=============-------------------------------------] 26% (1 threads)Inverse distance weighting: [=============-------------------------------------] 27% (1 threads)Inverse distance weighting: [==============------------------------------------] 28% (1 threads)Inverse distance weighting: [==============------------------------------------] 29% (1 threads)Inverse distance weighting: [===============-----------------------------------] 30% (1 threads)Inverse distance weighting: [===============-----------------------------------] 31% (1 threads)Inverse distance weighting: [================----------------------------------] 32% (1 threads)Inverse distance weighting: [================----------------------------------] 33% (1 threads)Inverse distance weighting: [=================---------------------------------] 34% (1 threads)Inverse distance weighting: [=================---------------------------------] 35% (1 threads)Inverse distance weighting: [==================--------------------------------] 36% (1 threads)Inverse distance weighting: [==================--------------------------------] 37% (1 threads)Inverse distance weighting: [===================-------------------------------] 38% (1 threads)Inverse distance weighting: [===================-------------------------------] 39% (1 threads)Inverse distance weighting: [====================------------------------------] 40% (1 threads)Inverse distance weighting: [====================------------------------------] 41% (1 threads)Inverse distance weighting: [=====================-----------------------------] 42% (1 threads)Inverse distance weighting: [=====================-----------------------------] 43% (1 threads)Inverse distance weighting: [======================----------------------------] 44% (1 threads)Inverse distance weighting: [======================----------------------------] 45% (1 threads)Inverse distance weighting: [=======================---------------------------] 46% (1 threads)Inverse distance weighting: [=======================---------------------------] 47% (1 threads)Inverse distance weighting: [========================--------------------------] 48% (1 threads)Inverse distance weighting: [========================--------------------------] 49% (1 threads)Inverse distance weighting: [=========================-------------------------] 50% (1 threads)Inverse distance weighting: [=========================-------------------------] 51% (1 threads)Inverse distance weighting: [==========================------------------------] 52% (1 threads)Inverse distance weighting: [==========================------------------------] 53% (1 threads)Inverse distance weighting: [===========================-----------------------] 54% (1 threads)Inverse distance weighting: [===========================-----------------------] 55% (1 threads)Inverse distance weighting: [============================----------------------] 56% (1 threads)Inverse distance weighting: [============================----------------------] 57% (1 threads)Inverse distance weighting: [=============================---------------------] 58% (1 threads)Inverse distance weighting: [=============================---------------------] 59% (1 threads)Inverse distance weighting: [==============================--------------------] 60% (1 threads)Inverse distance weighting: [==============================--------------------] 61% (1 threads)Inverse distance weighting: [===============================-------------------] 62% (1 threads)Inverse distance weighting: [===============================-------------------] 63% (1 threads)Inverse distance weighting: [================================------------------] 64% (1 threads)Inverse distance weighting: [================================------------------] 65% (1 threads)Inverse distance weighting: [=================================-----------------] 66% (1 threads)Inverse distance weighting: [=================================-----------------] 67% (1 threads)Inverse distance weighting: [==================================----------------] 68% (1 threads)Inverse distance weighting: [==================================----------------] 69% (1 threads)Inverse distance weighting: [===================================---------------] 70% (1 threads)Inverse distance weighting: [===================================---------------] 71% (1 threads)Inverse distance weighting: [====================================--------------] 72% (1 threads)Inverse distance weighting: [====================================--------------] 73% (1 threads)Inverse distance weighting: [=====================================-------------] 74% (1 threads)Inverse distance weighting: [=====================================-------------] 75% (1 threads)Inverse distance weighting: [======================================------------] 76% (1 threads)Inverse distance weighting: [======================================------------] 77% (1 threads)Inverse distance weighting: [=======================================-----------] 78% (1 threads)Inverse distance weighting: [=======================================-----------] 79% (1 threads)Inverse distance weighting: [========================================----------] 80% (1 threads)Inverse distance weighting: [========================================----------] 81% (1 threads)Inverse distance weighting: [=========================================---------] 82% (1 threads)Inverse distance weighting: [=========================================---------] 83% (1 threads)Inverse distance weighting: [==========================================--------] 84% (1 threads)Inverse distance weighting: [==========================================--------] 85% (1 threads)Inverse distance weighting: [===========================================-------] 86% (1 threads)Inverse distance weighting: [===========================================-------] 87% (1 threads)Inverse distance weighting: [============================================------] 88% (1 threads)Inverse distance weighting: [============================================------] 89% (1 threads)Inverse distance weighting: [=============================================-----] 90% (1 threads)Inverse distance weighting: [=============================================-----] 91% (1 threads)Inverse distance weighting: [==============================================----] 92% (1 threads)Inverse distance weighting: [==============================================----] 93% (1 threads)Inverse distance weighting: [===============================================---] 94% (1 threads)Inverse distance weighting: [===============================================---] 95% (1 threads)Inverse distance weighting: [================================================--] 96% (1 threads)Inverse distance weighting: [================================================--] 97% (1 threads)Inverse distance weighting: [=================================================-] 98% (1 threads)Inverse distance weighting: [=================================================-] 99% (1 threads)Inverse distance weighting: [==================================================] 100% (1 threads)",
    "crumbs": [
      "Home",
      "LiDAR Point Clouds"
    ]
  },
  {
    "objectID": "01-point-cloud.html#result",
    "href": "01-point-cloud.html#result",
    "title": "LiDAR Point Cloud Processing",
    "section": "3. Result",
    "text": "3. Result\n\nArea-Based Canopy Height Model\nA high-resolution Canopy Height Model (CHM) was derived from the height-normalized point cloud using the dsmtinalgorithm. The CHM provides a continuous raster representation of the forest canopy, which is a key product for visual assessment and quantitative analysis, and is a fundamental input for both area-based and individual tree-based methods.\n#| warning: false\n#| message: false\n#| error: false\n#| echo: true\n#| eval: false\n\nopt_selopt_output_files(las_ctg_ahbau_csf_sor_norm) = paste0(tempdir(), \"./Data/las_ctg_ahbau_dtm\")\nopt_select(las_ctg_ahbau_csf_sor_norm) = \"xyzr\"\nopt_filter(las_ctg_ahbau_csf_sor_norm) = '-keep_first' \nopt_chunk_size(las_ctg_ahbau_csf_sor_norm) = 1000 \nopt_chunk_buffer(las_ctg_ahbau_csf_sor_norm) = 10\nsensor(las_ctg_ahbau_csf_sor_norm) = 'als'\nindex(las_ctg_ahbau_csf_sor_norm) = \"auto\"\n\nlas_ctg_ahbau_chm = grid_canopy(las_ctg_ahbau_csf_sor_norm, 1, dsmtin(8))\nlas_tile_ahbau_chm = grid_canopy(las_tile_ahbau_csf_sor_norm, 1, dsmtin(8))\nplot(las_tile_ahbau_chm, col = height.colors(50))\n\n\n\n\nIndividual Tree Detection & Segmentation\nThis section outlines the process for identifying and segmenting individual tree crowns, which is the most accurate method for deriving metrics like stems/ha and stand height in forest mensuration.\n\n\nTree top detection\nIndividual trees were detected using the local maxima filter (lmf) algorithm. A key step in this process was the application of a custom window function, which uses a height-based allometric model to dynamically determine the search radius for each tree top. This method allows the model to adapt to variations in tree size across the landscape, leading to a more accurate detection. The uniqueness function was also used to prevent replicated trees from being counted across adjacent tiles.\n\n# window function (Quan, 2022)\nwf_Quan&lt;-function(x){ #so far these parameters are best will try 6m floor. \n  a=0.179-0.1\n  b=0.51+0.5 #Go big but not 2.49 big. maybe increase slope a hair.\n  y&lt;-a*x+b #y[x&lt;15]=2.2\n  return(y)}\nheights &lt;- seq(0,40,0.5)\nwindow &lt;- wf_proto(heights)\nplot(heights, window, type = \"l\",  ylim = c(0,12), xlab=\"point elevation (m)\", ylab=\"window diameter (m)\")\n\n# find trees\nlas_tile_ahbau_ttops &lt;- find_trees(las_tile_ahbau_csf_sor_norm, lmf(wf_Quan), uniqueness = \"bitmerge\")\nlas_tile_ahbau_ttops_sf = st_as_sf(las_tile_ahbau_ttops)\nproj4string(las_tile_ahbau_ttops) &lt;- defaultCRS\nproj4string(las_tile_ahbau_chm) &lt;- defaultCRS\nst_crs(las_tile_ahbau_ttops_sf) = defaultCRS\n\nmypalette&lt;-brewer.pal(8,\"Reds\")\nplot(las_tile_ahbau_chm, col = mypalette, alpha=0.6)\nplot(st_geometry(las_tile_ahbau_ttops_sf[\"treeID\"]), add=TRUE, cex = 0.001, pch=19, col = 'red', alpha=0.8)\n\n\n\n\n\n\n\nCrown Segmentation\nFollowing detection, individual tree crowns were segmented to delineate their exact boundaries. This is a critical step in a batch processing workflow, as it ensures that trees are not double-counted by multiple tiles. The dalponte2016 algorithm was used for this process, which assigns a unique ID to each point based on the segmented tree it belongs to.\n\nalgo = dalponte2016(las_tile_ahbau_chm, las_tile_ahbau_ttops)\nlas_tile_ahbau_segmented = segment_trees(las_tile_ahbau_csf_sor_norm, algo)\nlas_tile_ahbau_segmented_crowns = delineate_crowns(las_tile_ahbau_segmented, type = 'convex')\nlas_tile_ahbau_segmented_crowns_spplot = sp::plot(las_tile_ahbau_segmented_crowns, cex=0.000001, axes = TRUE, alpha=0.1)\n\n\n\n\n\nStems Per Hectare\nThis section explores two distinct approaches for deriving a stems/ha predictor. The point-based method leverages the accuracy of individual tree detection, while the raster-based method uses a simpler, aggregation-based approach.\n\n\nPoint-Based Method\nThe point-based approach is considered the most accurate method for deriving forest inventory metrics from LiDAR data. By building on the ITDS pipeline, this workflow derives a stemsha_L predictor by rasterizing the precise locations of the detected individual trees, providing a highly reliable and detailed stem map.\n\n#sp::spTransform(las_tile_ahbau_segmented_crowns) = defaultCRS\nlas_tile_ahbau_segmented_crowns_sf = st_as_sf(las_tile_ahbau_segmented_crowns, coords = c(\"XTOP\", \"YTOP\"), st_crs=defaultCRS)\npsych::describe(las_tile_ahbau_segmented_crowns_sf$treeID)\n\nraster_template = rast(ext(las_tile_ahbau_chm), resolution = 20, crs = st_crs(las_tile_ahbau_chm)$wkt)\nlas_tile_ahbau_ttops_rast = rasterize(vect(las_tile_ahbau_ttops_sf), raster_template, fun = sum, touches = TRUE)\nstemsha_L_rast = las_tile_ahbau_ttops_rast*5\ncrs(stemsha_L_rast) = defaultCRS\nplot(stemsha_L_rast)\nstemsha_L = raster::raster(stemsha_L_rast, filename = \"./Data/stemsha_L.tif\")\nstemsha_L = writeRaster(stemsha_L, filename = \"./Data/stemsha_L.tif\", overwrite=TRUE)\nplot(stemsha_L)\n\n\n\n\n\n\n\n\n\nRaster-Based Method\nAs an alternative, a stem map was also derived from the CHM using raster aggregation functions. While this method is computationally simpler, its accuracy is limited by the aggregation sampling, which can result in rounding or omission errors. For instance, in one analysis, the method resulted in a significant reduction in cell numbers (ncells: 1080 &gt; 330), indicating a loss of data integrity.\n\n# Point-to-raster 2 resolutions\nttops_chm_p2r_05 &lt;- locate_trees(chm_p2r_05, lmf(5))\n\nlas_tile_ahbau_chm = grid_canopy(las_tile_ahbau_csf_sor_norm, 1, dsmtin(8))\nplot(lead_htop_raster, col = height.colors(50))\n\nchm_p2r_05 &lt;- rasterize_canopy(las, 0.5, p2r(subcircle = 0.2), pkg = \"terra\")\nchm_p2r_1 &lt;- rasterize_canopy(las, 1, p2r(subcircle = 0.2), pkg = \"terra\")\n\n# Pitfree with and without subcircle tweak\nchm_pitfree_05_1 &lt;- rasterize_canopy(las, 0.5, pitfree(), pkg = \"terra\")\nchm_pitfree_05_2 &lt;- rasterize_canopy(las, 0.5, pitfree(subcircle = 0.2), pkg = \"terra\")\n\n# Post-processing median filter\nkernel &lt;- matrix(1,3,3)\nchm_p2r_05_smoothed &lt;- terra::focal(chm_p2r_05, w = kernel, fun = median, na.rm = TRUE)\nchm_p2r_1_smoothed &lt;- terra::focal(chm_p2r_1, w = kernel, fun = median, na.rm = TRUE)\n\n# chm to stem map pipeline\nlead_htop = rast(\"./Data/Raster_Covariates/lead_htop_raster.tif\")\nelev = rast(\"./Data/Raster_Covariates/elev_raster.tif\")\nelev = mask(elev, vect(aoi_sf))\nlead_htop = mask(lead_htop, vect(aoi_sf))\nlead_htop[lead_htop &lt; 1.3] &lt;- NA\nelev = mask(elev, lead_htop, inverse=FALSE)\nplot(lead_htop, main = \"canopy height (m)\")\nplot(elev, main = \"elevation\")\n\n\n\n95% Canopy Height Adjustment\n\n{r}\n\nError: object 'r' not found\n\n#| warning: false\n#| message: false\n#| error: false\n#| echo: true\n#| eval: false\n\n# custom function writing\nquant95 &lt;- function(x, ...) quantile(x, c(.95), na.rm = TRUE)\ncustFuns &lt;- list(quant95, max)\nnames(custFuns) &lt;- c(\"95thQuantile\", \"Max\")\n\n\n#raster post-process smoothing\nkernel &lt;- matrix(1,3,3)\nterra::focal(lead_htop_rast, w = kernel, fun = median, na.rm = TRUE)\nlead_htop_raster = raster(lead_htop)\n#foretTools pipeline\nttops_6m_proto = vwf(CHM = lead_htop_raster, winFun = wf_proto, minHeight = 6)\nttops_6m_Plowright = vwf(CHM = lead_htop_raster, winFun = wf_Plowright, minHeight = 6)\nttops_6m_wf5 = vwf(CHM = lead_htop_raster, winFun = 5, minHeight = 6)\nttops_2m_proto = vwf(CHM = lead_htop_raster, winFun = wf_proto, minHeight = 2)\nttops_2m_Plowright = vwf(CHM = lead_htop_raster, winFun = wf_Plowright, minHeight = 2)\n# window size mining\nheight_crown_cor = lm(ttops_6m_proto$height ~ ttops_6m_proto$winRadius)\nheight_crown_cor \n\n# custom functions insert here\nsp_summarise(ttops_6m_proto, areas = vri_aoi_sf, variables = \"height\", statFuns = custFuns)\nsp_summarise(ttops_6m_proto, areas = vri_aoi_sf, statFuns = custFuns) #treecount\n\n# rasterized results at 50m resolution\ngridStats &lt;- sp_summarise(ttops_6m_proto = ttops, grid = 50, variables = \"height\")\n\n#lidr pipeline\nttops_6m_proto = lidR::find_trees(lead_htop_raster[lead_htop_raster&gt;6], lmf(wf_Quan))\nttops_6m_Plowright = lidR::find_trees(lead_htop_raster[lead_htop_raster&gt;6], lmf(wf_Plowright))\nstemsha_L_sf = st_as_sf(ttops_6m_proto)\nstemsha_L_raster = raster::raster(stemsha_L_sf)\n# visualize\nmypalette&lt;-brewer.pal(8,\"Greens\")\n#plot(lead_htop_raster, col = mypalette, alpha=0.6)\n#plot(st_geometry(stemsha_L[\"treeID\"]), add=TRUE, cex = 0.001, pch=19, col = 'red', alpha=0.8)\nwriteRaster(stemsha_L_raster, filename = \"./Data/Raster_Covariates/stemsha_L_raster.tif\", overwrite=TRUE)\nplot(stemsha_L_raster)",
    "crumbs": [
      "Home",
      "LiDAR Point Clouds"
    ]
  },
  {
    "objectID": "01-point-cloud.html#footnotes",
    "href": "01-point-cloud.html#footnotes",
    "title": "LiDAR Point Cloud Processing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nReminder: Run individual las_check on two chunks showing warnings to derive image report of grid_terrain… knnidw() operation in the bottom image↩︎",
    "crumbs": [
      "Home",
      "LiDAR Point Clouds"
    ]
  }
]